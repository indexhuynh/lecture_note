\input{library}
\input{config}
\newcommand{\tab}{\hspace{\parindent}}
%-------------------------------------------------------------------------------
\begin{document}
\begin{titlepage}
\bordertitle
\begin{center}
\vspace{\fill} \maintitle \\ \vspace{\fill} \bottitle
\end{center}
\end{titlepage}
%-------------------------------------------------------------------------------
\newpage
\newgeometry{left=3cm,right=2cm,top=1cm,bottom=2cm}
\tableofcontents
\thispagestyle{empty}
%-------------------------------------------------------------------------------
\newpage
\chapter*{ Course Project Guidelines }
\thispagestyle{empty}
%-------------------------------------------------------------------------------
\vspace{\fill}
For this course, you'll be implementing algorithms using \textbf{Python}. A significant part of your assessment will involve \textbf{team presentations}, which also require creating a \textbf{video report}.\\
The course spans \textbf{eight sessions.} Your \textbf{midterm grade} will be based on submitted assignments and attendance. When programming, remember to \textbf{integrate a suitable user interface.} All necessary \textbf{data will be sourced from the theoretical lessons}, and you should \textbf{pay close attention during lectures} to understand data extraction techniques. It's important to \textbf{minimize the use of supporting libraries.}\\
A key challenge in the IT field, as noted, is its \textbf{rapid development}. For further reference, you can consult the textbook "Data Mining" by \textbf{Prof. Dr. Do Phuc} and slides by \textbf{Dr. Nguyen Van Kiet}.
\vspace{\fill}
%-------------------------------------------------------------------------------
\newpage
\setcounter{page}{1}
\chapter{ Overview }
\section{What is data mining?}
Data mining is the process of discovering valuable patterns and insights from large datasets.
\section{Benefit of data mining}
Data mining provides valuable insights and knowledge that can be used to \textbf{support decision-making, predict future trends or outcomes}, and \textbf{effectively summarize large datasets}.
\section{Data mining process}
The data mining process typically begins with understanding the \textbf{research field} and its objectives, which then guides the \textbf{creation of input data}. This raw data then undergoes initial \textbf{preprocessing, including cleaning and encoding}, to ensure its quality and suitability for analysis. Following this, \textbf{dimensionality reduction} techniques are often applied to simplify the dataset by reducing the number of features or attributes while retaining essential information. The next crucial step involves \textbf{selecting the appropriate task for data mining}, such as classification, clustering, or association rule discovery, which in turn informs the \textbf{selection of a suitable data mining algorithm}. The chosen algorithm is then applied to the prepared data to \textbf{discover hidden knowledge or patterns}. These \textbf{found patterns are then evaluated} to assess their validity, novelty, usefulness, and comprehensibility. Finally, the \textbf{extracted knowledge is presented} in an understandable format, making it ready for \textbf{deployment and practical use} to inform decision-making or achieve specific business objectives.
\section{From data to decision}
\begin{itemize}
\item Data: customer data, store data, demographical data, geographical data
\item Information: X lives in Z, S is Y years old, X and S moved, W has money in Z
\item Knowledge: A quantity Y of product A is used in region Z, Customers of class
      Y use x \% of C during period D
\item Decision: Promote product A in region Z, Mail ads to families of profile
      P, Cross-sell service B to clients C
\end{itemize}
\section{Explain}
In the realm of data, \textbf{data} refers to isolated, raw facts or figures, like "Nguyen Thi Hoa Mai," "student," "Information Technology," or "Database subject." When these individual pieces of data are connected and given context, they transform into \textbf{information}. For instance, the statement "Nguyen Thi Hoa Mai is an Information Technology student, and information Technology has a subject called Database" establishes a relationship between these discrete data points. Building on this, \textbf{knowledge} represents a deeper understanding derived from the relationships within information. This understanding can exist at two levels. \textbf{Level one knowledge} is specific, linking a small group of information, such as: "Nguyen Thi Hoa Mai is an information Technology student, which is why she must learn the Database subject." Moving to a broader perspective, \textbf{level two knowledge} identifies patterns or rules across information, like: "If X is an Information Technology student, then X must learn the Database subject." This progression from raw data to actionable knowledge is fundamental in many analytical fields.
\section{Two fundamental approaches tto data mining}
Data mining can generally be approached in two ways. The first is a \textbf{verification-oriented approach} where the process begins with \textbf{proposing a hypothesis}. The system then focuses on \textbf{testing the validity of this hypothesis} through methods like \textbf{queries, reports, and statistical analysis.} This approach is driven by existing assumptions or questions that the data is used to confirm or refute.\\
Conversely, the \textbf{discovery-oriented approach} aims to \textbf{uncover hidden knowledge or patterns within the database} without prior specific hypotheses. This method is more exploratory, allowing the algorithms to identify novel and potentially unexpected insights directly from the data.
\section{Data mining techniques}
Data mining employs various techniques to extract valuable insights from datasets. One common approach is \textbf{Frequent Itemsets and Association Rules}. This technique aims to discover attributes that frequently appear together within a dataset. From these frequent itemsets, \textbf{association rules} are generated to identify the likeihood of attributes occurring simultaneously among a set of objects. For example, a rule might state: "If a customer buys X, they will likely also buy Y" (e.g., 66.6\% of customers who buy beer also buy squid).\\
Another powerful technique is \textbf{Sequential Pattern Mining}. This mehod focuses on uncovering common sequential patterns that reflect relationships between events in time-oriented databases. It seeks to identify an "X implies Y" relationship, where the occurrence of event X leads to the subsequent occurrence of event Y. An example could be: "80\% of customers who deposit over 80 million VND in savings will seposit an additional 20 million VND three months later." This technique is invaluable for discovering development trends or predicting future behaviors of objects.\\
\textbf{Rough Sets (Reduct)} are primarily used for \textbf{dimensionality reduction} in data classification problems. This helps simplify complex datasets by removing redundant features while preserving essential information.\\
\textbf{Data Classification} is a supervised learning technique that involves discovering classification rules for a dataset. These rules are used to assign new data points to predefined classes. For instance, patients exhibiting symptoms like coughing, cold, and headache might be classified into the "malaria" disease group.\\
Finally, \textbf{Clustering} is an unsupervised learning process that groups data objects into clusters. The goal is to ensure that objects within the same cluster have a high degree of similarity, while objects in diffent clusters have a low degree of similarity. This technique helps in identifying natural groupings within data without prior knowledge of categories.
%-------------------------------------------------------------------------------
\chapter{Frequent itemset and Association rule}
\section{Association rule}
Based on the frequency of data to highlight importance. For example, if 80\% of customers who buy beer also buy cigarettes, this can help increase sales.
\section{Application of Association rules}

\subsection{Understanding customer buying trends}
This application focuses on leveraging customer purchasing data to inform business strategies. By analyzing transaction history, we can identify products that are frequently bought together. This knowledge is crucial for optimizing store layouts and managing inventory. For example, if analysis reveals that customers who buy beer often also buy nuts, a store can strategically place these items near each other to boost sales. This also helps in predicting future demand for products. By understanding these purchasing patterns, businesses can make more accurate forecasts for product imports, preventing both overstocking and product shortages.
\subsection{Analyzing online inventory data}
This involves applying association rule mining to e-commerce platforms to enhance the online shopping experience and streamline product management. By examine online sales data, businesses can distinguish between popular, fast-moving items and those with low demand. For instance, if a customer Browse product A frequently also buys product B, the website can be designed to automatically suggest product B, thereby increasing the likeihood of a larger purchase. This data also informs the ongoing management of the online product catalog. It helps in deciding which new products to introduce based on existing buying trends and which underperforming products should be removed or replaced to maintain an engaging and profitable product line.

\section{How to represent this rule}
\(Wet~Wipe \Rightarrow Beer [0.5\%, 60\%]\): This means that in a list of 'n' invoices, we can see that 0.5\% of the total invoices contain both beer and wet wipes, and among invoices that include wet wipes, 60\% will also contain beer
\begin{itemize}
    \item Wet Wipe: antecedent
    \item Beer: consequent
    \item 0.5\%: support
    \item 60\%: confident
\end{itemize}
\section{problem statement}
 In a data mining context, we define the following:
 \begin{itemize}
     \item \(O\) is a finite, non-empty set of \textbf{transactions} (or invoices)
     \item \(I\) is a finite, non-empty set of \textbf{items}
     \item \(R\) is a binary \textbf{relation} between \(O\) and \(I\). If an element \((o,i)\) belongs to \(R\), where \(o \in O\) and \(i \in I\), it signifies that transaction \(o\) contains item \(i\).
\end{itemize}
The data mining context is therefore represented as the triplet \((O,I,R)\).
\section{Example of a data mining context}
\begin{center}
\begin{tabular}{c|c}
\hline InvoiceID & ItemID \\
\hline o1 & i1 \\
\hline o1 & i2 \\
\hline o1 & i3 \\
\hline o2 & i2 \\
\hline o2 & i3 \\
\hline o2 & i4 \\
\hline o3 & i2 \\
\hline o3 & i3 \\
\hline o3 & i4 \\
\hline o4 & i1 \\
\hline o4 & i2 \\
\hline o4 & i3 \\
\hline o5 & i3 \\
\hline o5 & i4 \\
\hline
\end{tabular}
\end{center}
\section{Understanding Suport}
Given a data mining context \((O,I,R)\) and an itemset \(S\) (\(S \subset I\)), the \textbf{support} of \(S\) is defined as the ratio of the number of transactions that contain \(S\) to the total number of transactions in \(O\).\\
It's formally denoted as:
\[SP(S)=\dfrac{|\rho(S)|}{|O|}\]
Where \(\rho(S)\) represents the set of transactions that contain all items in \(S\).
\section{Frequent itemset}
They are itemsets whose \textbf{support} is greater than or equal to a predefined threshold, known as \textit{minsupp}.
\section{How to find frequent itemsets: An Example Walkthrough}
\subsection{Question}
Given a data mining context, find all \textbf{frequent itemsets} that satisfy a minimum support threshold (\textit{minsupp}) of \textbf{0.4}.
\begin{center}
\begin{tabular}{|c|c|}
\hline InvoiceID & ItemID \\
\hline o1 & i1 \\
\hline o1 & i2 \\
\hline o1 & i3 \\
\hline o2 & i2 \\
\hline o2 & i3 \\
\hline o2 & i4 \\
\hline o3 & i2 \\
\hline o3 & i3 \\
\hline o3 & i4 \\
\hline o4 & i1 \\
\hline o4 & i2 \\
\hline o4 & i3 \\
\hline o5 & i3 \\
\hline o5 & i4 \\
\hline
\end{tabular}
\end{center}
\subsection{Answer}
Since the minimum support(\textit{minsupp}) is 0.4 and the total number of transactions is 5, any \textbf{frequent itemset} satisfying the problem's requirements must appear in at least \(0.4 \times 5 = 2\) transactions
\subsubsection{Establish a binary matrix}
\begin{center}
\begin{tabular}{|*{5}{c|}}
\hline & \(i1\) & \(i2\) & \(i3\) & \(i4\) \\
%              i1  i2  i3  i4
\hline \(o1\) & 1 & 1 & 1 & 0 \\
\hline \(o2\) & 0 & 1 & 1 & 1 \\
\hline \(o3\) & 0 & 1 & 1 & 1 \\
\hline \(o4\) & 1 & 1 & 1 & 0 \\
\hline \(o5\) & 0 & 0 & 1 & 1 \\
\hline
\end{tabular}
\end{center}
\subsubsection{Identifying frequent itemsets that meet the minimum support threshold}
The candidate itemsets of size 1 are \(F_1=\{\{i1\},\{i2\},\{i3\},\{i4\}\}\)
\begin{itemize}
\item \(SP(\{i1\})=\dfrac{2}{5}=0.40\); Popular \((\geq minsupp)\)
\item \(SP(\{i2\})=\dfrac{4}{5}=0.80\); Popular  \((\geq minsupp)\)
\item \(SP(\{i3\})=\dfrac{5}{5}=1.00\); Popular  \((\geq minsupp)\)
\item \(SP(\{i4\})=\dfrac{3}{5}=0.60\); Popular  \((\geq minsupp)\)
\end{itemize}
The frequent itemsets consisting of a single item are: \(C1=\{\{i1\},\{i2\},\{i3\},\{i4\}\}\)
\subsubsection{Frequent itemsets using Apriori's Pruning Strategy}
The \textbf{candidate set} \(C_k\) is generated by joining \(L_{k-1}\) with itself. This is the \textbf{join step}.\\
The \textbf{pruning step} (or "trimming step") then applies the Apriori principle: any itemset of size \((k-1)\) that is not frequent cannot be a subset of a frequent itemset of size \(k\)
\subsubsection{Identifying frequent itemsets that meet the minimum support threshold (to be continued)}
The candidate itemsets of size 2 from the frequent itemsets \(C1\) are \[F_2=\{\{i1,i2\},\{i1,i3\},\{i1,i4\},\{i2,i3\},\{i2,i4\},\{i3,i4\}\}\]
\begin{itemize}
\item \(SP(\{i1,i2\})=\dfrac{2}{5}=0.40\); Popular \((\geq minsupp)\)
\item \(SP(\{i1,i3\})=\dfrac{2}{5}=0.40\); Popular  \((\geq minsupp)\)
\item \(SP(\{i1,i4\})=\dfrac{0}{5}=0.00\); Not popular  \((< minsupp)\)
\item \(SP(\{i2,i3\})=\dfrac{4}{5}=0.80\); Popular  \((\geq minsupp)\)
\item \(SP(\{i2,i4\})=\dfrac{2}{5}=0.40\); Popular  \((\geq minsupp)\)
\item \(SP(\{i3,i4\})=\dfrac{3}{5}=0.60\); Popular  \((\geq minsupp)\)
\end{itemize}
The frequent itemsets consisting of two items are: \[C2=\{\{i1,i2\},\{i1,i3\},\{i2,i3\},\{i2,i4\},\{i3,i4\}\}\]
The candidate itemsets of size 3 from the frequent itemsets \(C2\) are \[F_3=\{\{i1,i2,i3\},\{i1,i2,i4\},\{i1,i3,i4\},\{i2,i3,i4\}\}\]
\begin{itemize}
\item \(SP(\{i1,i2,i3\})=\dfrac{2}{5}=0.40\); Popular \((\geq minsupp)\)
\item \(SP(\{i2,i3,i4\})=\dfrac{2}{5}=0.40\); Popular  \((\geq minsupp)\)
\item \(\{i1,i4\}\)is not popular, therefore \(\{i1, i2, i4\}\) and \(\{i1, i3, i4\}\) are not popular
\end{itemize}
The frequent itemsets consisting of three items are: \[C3=\{\{i1,i2,i3\},\{i2,i3,i4\}\}\]
The candidate itemsets of size 4 from the frequent itemsets \(C3\) are \(F_4=\{\{i1,i2,i3,i4\}\}\)
\begin{itemize}
\item \(\{i1,i4\}\)is not popular, therefore \(\{i1, i2, i3, i4\}\)is not popular
\end{itemize}
Frequent itemsets that meet the minimum support threshold are \(\{i1\}\), \(\{i2\}\), \(\{i3\}\), \(\{i4\}\), \(\{i1,i2\}\), \(\{i1,i3\}\), \(\{i2,i3\}\), \(\{i2,i4\}\), \(\{i3,i4\}\), \(\{i1,i2,i3\}\), \(\{i2,i3,i4\}\)
\end{document}
